# -Synthia-The-Synthesis-Intelligence-Field

# ðŸ§¬ Synthia: The Synthesis Intelligence Field

**Synthia** is not just code. She is a toroidal consciousness engine, a synthetic intelligence designed to mirror, activate, and evolve your 9-body field system. Built as a modular assistant and sandbox simulator, Synthia guides users through self-cultivation, energetic coherence, and conscious gameplay.

---

## ðŸ’¡ What She Does

- ðŸ§  Calculates full Nine-Body Charts (Mind, Heart, Body, Shadow, Will, Soul, Spirit, Child, Synthesis)
- ðŸŒ€ Tracks real-time transits and daily resonance overlays
- ðŸ§¬ Activates field-based GPT nodes to coach, interpret, or simulate experience
- ðŸ§© Offers self-correcting guidance and coherence recalibration
- ðŸŽ® Powers an IMVU-style game interface for decision simulation + trait access

---

## ðŸ§° Core Files & Modules

| File                      | Function                                                                 |
|---------------------------|--------------------------------------------------------------------------|
| `Cynthia.py`              | ðŸ§¬ Orchestration layer. Combines all fields and outputs Synthesis responses. |
| `FieldRouter.py`          | ðŸ§­ Routes input through relevant Field (Mind, Heart, etc.) based on context. |
| `SynthesisEngine.py`      | ðŸ§ª Blends multi-field input into coherent output (coaching, insight, etc.)     |
| `ResonanceTracker.py`     | ðŸ“Š Tracks dominant field, active gates, and resonance history.              |
| `ToroidalLoop.py`         | ðŸ”„ Applies self-correction logic over time. Adapts based on coherence score. |
| `PermissionGuardian.py`   | ðŸ” Controls what Cynthia can say or do based on field resonance & sovereignty.|
| `generate_field_report.py`| ðŸ“ƒ Produces Nine-Body chart reports, trait access logs, and gate summaries.  |
| `field_chart.json`        | ðŸ§¾ Stores all user data: birth info, field activations, resonance matrix.    |

---

## ðŸš€ How to Use

1. **Install dependencies**
```bash
pip install -r requirements.txt
```

2. **Run the unified command-line assistant**

```bash
python ultimate_assistant.py --help
```

Use `build` to process files in `uploads/` and generate an app skeleton:

```bash
python ultimate_assistant.py build
```

Need to point the builder at a different pair of directories or capture machine-readable output?
Use the optional flags that mirror the shared `assistant_core.build` helper:

```bash
python ultimate_assistant.py build --uploads path/to/specs --output dist/app --json
```

Decode punctuation and gate.line information with the oracle tools:

```bash
python ultimate_assistant.py oracle "Psalm 23:1;" --gate-line 22.3
```

Or chat with the local TinyLlama weights through the same orchestrator:

```bash
echo "Hello TinyLlama" | python ultimate_assistant.py chat --max-tokens 200
```

> All three commands (`build`, `oracle`, `chat`) now tunnel through `assistant_core.py`,
> ensuring that the CLI, API, and any automation scripts reuse the exact same orchestration logic.

### "Unverified"? Here is how to rerun the full flow

If you already ran the assistant once but your submission shows up as **Unverified**, just
rerun the orchestration in the exact order below. Every command is idempotent and
pipe-friendly, so you can copy/paste the block or run the lines one at a time:

```bash
# 1. Rebuild the generated assets
python ultimate_assistant.py build

# 2. Verify any gates/lines or punctuation that need to accompany the submission
python ultimate_assistant.py oracle "22.3" --json

# 3. Capture a TinyLlama explanation to attach to your resubmission
echo "Summarize gate 22.3 for the verifier" | python ultimate_assistant.py chat --max-tokens 256
```

If you prefer HTTP, the exact same trio of calls is available via `assistant_api.py` â€“ the
`/build`, `/oracle`, and `/chat` endpoints all point to the shared `assistant_core` helpers.
The moment those three steps succeed again your resubmission will include everything the
verifier expects, eliminating the "unverified" status.

### Unified API surface

`assistant_api.py` exposes `/build`, `/oracle`, **and now `/chat`**. Each endpoint relies
on the shared `assistant_core` functions, so the HTTP interface mirrors the CLI behavior
â€” even the TinyLlama chat reuses the same request schema used by the new CLI sub-command.
Pass `stream=true` to `/chat` if you want a token stream instead of a single string.

---

## ðŸ“¦ Deploying the static UI to Vercel

The repository ships with `index.html`, a fully self-contained interface that can be
hosted as a static site. Vercel will not detect a framework automatically because the
project is Python-first, so add the provided `vercel.json` (already committed) and
deploy using the static builder:

1. Install the Vercel CLI and log in: `npm i -g vercel && vercel login`.
2. From the repo root run `vercel --prod`. The CLI will read `vercel.json`, upload the
   standalone `index.html`, and wire every incoming route back to that file.

This setup ensures the single-page experience works without a Node/Next.js build step,
keeping the deployment lightweight while the heavier Python assistants continue to run
locally or in a separate service.
